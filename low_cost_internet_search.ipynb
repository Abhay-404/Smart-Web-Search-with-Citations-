{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0cb4d957f1c84520bb89d58823797c8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0a4e191701c64005b72553cde9a6861e",
              "IPY_MODEL_e2f07c0962a34068bfedc2a24fb57a5a"
            ],
            "layout": "IPY_MODEL_2ac6eb54d3c04090ae102a14b3b8889b"
          }
        },
        "0a4e191701c64005b72553cde9a6861e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Query:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_3df0c98f71da4757801269400be3c504",
            "placeholder": "Enter your question here...",
            "style": "IPY_MODEL_633ad8f8a5da4c4496dd97009cc6b5b7",
            "value": ""
          }
        },
        "e2f07c0962a34068bfedc2a24fb57a5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "primary",
            "description": "Search",
            "disabled": false,
            "icon": "search",
            "layout": "IPY_MODEL_cd27f83537c3414495f819b386f5cf7b",
            "style": "IPY_MODEL_bffb385eacc844ceab3267001fb403e2",
            "tooltip": ""
          }
        },
        "2ac6eb54d3c04090ae102a14b3b8889b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3df0c98f71da4757801269400be3c504": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "80%"
          }
        },
        "633ad8f8a5da4c4496dd97009cc6b5b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd27f83537c3414495f819b386f5cf7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bffb385eacc844ceab3267001fb403e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "9316c7f7741848a28909d5a5f2c08c4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Show debug info",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_33a7bc344b8540d0b1d719e99320d8cc",
            "style": "IPY_MODEL_0483e62f41ec459bb7a85751848b8b17",
            "value": false
          }
        },
        "33a7bc344b8540d0b1d719e99320d8cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "auto"
          }
        },
        "0483e62f41ec459bb7a85751848b8b17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e831c48a37014a1e915d3ed7c68e68e7": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_d1b5e6e3d91f49af9a62c727889d8eb0",
            "msg_id": "",
            "outputs": []
          }
        },
        "d1b5e6e3d91f49af9a62c727889d8eb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "377a9c92b6f1421faa4da00d4d58bf15": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_f4b543382e7c4483b57c354663ae2ed7",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.HTML object>",
                  "text/html": "\n                <div class=\"result-container\">\n                    <h2>Answer</h2>\n                    <div class=\"answer-box\"><p>Here's a summary of the top trending news stories in India on March 29, 2025, based on the provided search results:</p>\n\n<ul>\n<li><strong>Sports:</strong> Nihal Sarin won the Tashkent Open, Manav Thakkar achieved victory in table tennis, and Gurindervir Singh became India's fastest man (<a href=\"https://indianexpress.com/archive/2025/03/29/\">https://indianexpress.com/archive/2025/03/29/</a>). Royal Challengers Bengaluru defeated Chennai Super Kings in cricket (<a href=\"https://www.thehindu.com/news/the-hindu-morning-digest-march-29-2025/article69388943.ece\">https://www.thehindu.com/news/the-hindu-morning-digest-march-29-2025/article69388943.ece</a>).</li>\n<li><strong>Politics and Economy:</strong> DMK protested against the central government over delayed funds for the 100-day work scheme (<a href=\"https://tamil.abplive.com/news/india/top-10-news-headlines-today-march-29-cm-stalin-usa-trump-dmk-tn-politics-ipl-2025-csk-vs-rcb-219832\">https://tamil.abplive.com/news/india/top-10-news-headlines-today-march-29-cm-stalin-usa-trump-dmk-tn-politics-ipl-2025-csk-vs-rcb-219832</a>). India and the U.S. held trade agreement talks (<a href=\"https://www.thehindu.com/news/the-hindu-morning-digest-march-29-2025/article69388943.ece\">https://www.thehindu.com/news/the-hindu-morning-digest-march-29-2025/article69388943.ece</a>). Discussions surrounding GST distribution, the National Education Policy (NEP) 2020, and judicial reforms were also prominent (<a href=\"https://www.nextias.com/ca/headlines-of-the-day/29-03-2025/headlines-of-the-day-29-3-2025\">https://www.nextias.com/ca/headlines-of-the-day/29-03-2025/headlines-of-the-day-29-3-2025</a>).</li>\n<li><strong>International Relations:</strong> India sent relief to earthquake-hit Myanmar (<a href=\"https://www.thehindu.com/news/the-hindu-morning-digest-march-29-2025/article69388943.ece\">https://www.thehindu.com/news/the-hindu-morning-digest-march-29-2025/article69388943.ece</a>, <a href=\"https://tamil.abplive.com/news/india/top-10-news-headlines-today-march-29-cm-stalin-usa-trump-dmk-tn-politics-ipl-2025-csk-vs-rcb-219832\">https://tamil.abplive.com/news/india/top-10-news-headlines-today-march-29-cm-stalin-usa-trump-dmk-tn-politics-ipl-2025-csk-vs-rcb-219832</a>). India and the US were jointly designing and manufacturing nuclear reactors in India (<a href=\"https://www.nextias.com/ca/headlines-of-the-day/29-03-2025/headlines-of-the-day-29-3-2025\">https://www.nextias.com/ca/headlines-of-the-day/29-03-2025/headlines-of-the-day-29-3-2025</a>).</li>\n<li><strong>Other:</strong> A drug-smuggling bust occurred in Tripura, and a fatal accident took place in Mumbai (<a href=\"https://indianexpress.com/archive/2025/03/29/\">https://indianexpress.com/archive/2025/03/29/</a>).</li>\n</ul>\n</div>\n                \n                    <p><em>Researched answer (took 16.41 seconds)</em></p>\n                </div>\n                "
                },
                "metadata": {}
              }
            ]
          }
        },
        "f4b543382e7c4483b57c354663ae2ed7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "608af66e996f4e389d55e08aa02971b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7d80706405e74dbeb91a3204ede941ec",
              "IPY_MODEL_b819673a673e490fa99a9f6d639ed82a"
            ],
            "layout": "IPY_MODEL_46d65d987b094e39901960d20f09686f"
          }
        },
        "7d80706405e74dbeb91a3204ede941ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Query:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_914b244d319840f3b6674df319395b73",
            "placeholder": "Enter your question here...",
            "style": "IPY_MODEL_88c397912b834b5197bf20df24a0f8a7",
            "value": ""
          }
        },
        "b819673a673e490fa99a9f6d639ed82a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "primary",
            "description": "Search",
            "disabled": false,
            "icon": "search",
            "layout": "IPY_MODEL_b1e9c5fc190c4f5d8e701b6c69e2dbb4",
            "style": "IPY_MODEL_b7b827ffc5e14b8c90e170b85e2de3d4",
            "tooltip": ""
          }
        },
        "46d65d987b094e39901960d20f09686f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "914b244d319840f3b6674df319395b73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "80%"
          }
        },
        "88c397912b834b5197bf20df24a0f8a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1e9c5fc190c4f5d8e701b6c69e2dbb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7b827ffc5e14b8c90e170b85e2de3d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "0391294194814088a4ffc0389edf2efe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Show debug info",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_f4d8be538b804f09b5124cde57cb5256",
            "style": "IPY_MODEL_f7d797db76a4403ca58118d15f90d5dd",
            "value": false
          }
        },
        "f4d8be538b804f09b5124cde57cb5256": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "auto"
          }
        },
        "f7d797db76a4403ca58118d15f90d5dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4fefe1100ada4df8be3c09a20df6e35c": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_05ced2337fc24335af44f073287fd27a",
            "msg_id": "",
            "outputs": []
          }
        },
        "05ced2337fc24335af44f073287fd27a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af4556fe8e0d4906ad6ff7f306a6ca59": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_d4fbd28a15d5471b9b62e180bc724aee",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.HTML object>",
                  "text/html": "\n                <div class=\"result-container\">\n                    <h2>Answer</h2>\n                    <div class=\"answer-box\"><p>Here's a summary of recent news headlines and developments in 2025:</p>\n\n<h2>Top News Headlines (Late March 2025)</h2>\n\n<ul>\n<li><strong>Global Issues:</strong> The UN is urging countries to reconsider using GDP as the sole measure of growth and has condemned an attack on peacekeepers in the Central African Republic. A major earthquake in Myanmar has resulted in over 1,600 deaths. The UN human rights chief has described the situation in Haiti as a \"catastrophe\" <a href=\"https://www.globalissues.org/news/2025\">https://www.globalissues.org/news/2025</a>.</li>\n<li><strong>TikTok's Future in the U.S.:</strong> TikTok faces a potential ban in the U.S. if ByteDance doesn't sell its U.S. operations by April 5th. Donald Trump has hinted at a possible deal, with companies like Perplexity AI, Frank McCourt's group, and Microsoft expressing interest in acquiring TikTok <a href=\"https://92q.com/playlist/the-biggest-news-stories-of-2025/\">https://92q.com/playlist/the-biggest-news-stories-of-2025/</a>.</li>\n</ul>\n\n<h2>Latest Developments</h2>\n\n<ul>\n<li><strong>Global Trends:</strong> Expect potential trade friction from Trump's tariff proposals, slower economic growth, and rising global tensions. Musk's ventures (\"Muskonomics\") and quantum computing breakthroughs are expected to grow. Green molecules and energy storage systems will transform energy. Robotaxis and air taxis may become more common, while biotech and precision fermentation revolutionize food and medicine. Spatial computing will expand, and bots will become more influential online <a href=\"https://www.forbes.com/sites/sarwantsingh/2025/01/22/top-15-global-trends-for-2025/\">https://www.forbes.com/sites/sarwantsingh/2025/01/22/top-15-global-trends-for-2025/</a>.</li>\n<li><strong>Sustainable Development Goals:</strong> The UN is encouraging countries to look beyond GDP when measuring progress toward Sustainable Development Goals <a href=\"https://www.globalissues.org/news/2025\">https://www.globalissues.org/news/2025</a>.</li>\n<li><strong>Humanitarian Crises:</strong> Besides the attack on UN peacekeepers and the earthquake in Myanmar, there are concerns about water insecurity, climate extremes in Latin America, child mortality, and the situation in Least Developed Countries. Haiti is facing a \"catastrophe\" due to gang violence <a href=\"https://www.globalissues.org/news/2025\">https://www.globalissues.org/news/2025</a>.</li>\n</ul>\n</div>\n                \n                    <p><em>Researched answer (took 14.95 seconds)</em></p>\n                </div>\n                "
                },
                "metadata": {}
              }
            ]
          }
        },
        "d4fbd28a15d5471b9b62e180bc724aee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Searching the query"
      ],
      "metadata": {
        "id": "seCVq-oLovtc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **First Problem:** Grounding is very expensive, and other solutions are also costly if not cached.  \n",
        "- **Second Problem:** I used DuckDuckGo to retrieve search results, but the API provides very short responses that do not fully cover the question.  \n",
        "- **Solution:** Use BeautifulSoup to extract the full body from the retrieved links and then summarize it using Flash 1.5/2.0.\n",
        "\n",
        "abhi bhi many problem to think(caching--> easy , rerank -> ? ,"
      ],
      "metadata": {
        "id": "bOHBZgZMvWhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q \"google-generativeai>=0.8.2\""
      ],
      "metadata": {
        "id": "Wl3NAev9cqqD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install duckduckgo-search markdown2 backoff\n"
      ],
      "metadata": {
        "id": "SoG-LJrGnjKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial Query (Using flash)"
      ],
      "metadata": {
        "id": "lI7WHOa7qex2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import google.generativeai as genai\n",
        "from datetime import datetime\n",
        "\n",
        "def process_query(user_query):\n",
        "    # Configure the API\n",
        "    api_key=\"\"\n",
        "    genai.configure(api_key=api_key)  # use gemini api key from https://aistudio.google.com/apikey\n",
        "\n",
        "    # Define system instruction to determine if search is needed\n",
        "    system_instruction = \"\"\"\n",
        "    Today Date = 29-03-2025\n",
        "    You are an assistant that determines if a query requires internet search. Analyze the query and return a JSON with:\n",
        "    1. \"needs_search\": boolean - true if the query requires recent information (after November 2023) or specific facts\n",
        "    2. \"reason\": string - brief explanation why search is/isn't needed\n",
        "    3. \"atomic_questions\": array - if query is complex, break it into smaller atomic questions (empty if search not needed)\n",
        "\n",
        "    IMPORTANT: Any query about events, products, news, or data after November 2023 MUST have \"needs_search\" set to true.\n",
        "    If you are not familar with the term asked in the question then also turn \"needs_search\" to true.\n",
        "    For small query return the original query. For complex and long queries requiring search, break them down into simpler 2-3 sub-questions.\n",
        "    Whenever someone ask about recent add 2025 in the sub questions.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create the model with appropriate configuration\n",
        "    generation_config = {\n",
        "        \"temperature\": 0.2,\n",
        "        \"top_p\": 0.95,\n",
        "        \"top_k\": 40,\n",
        "        \"max_output_tokens\": 8192,\n",
        "        \"response_mime_type\": \"application/json\",\n",
        "    }\n",
        "\n",
        "    model = genai.GenerativeModel(\n",
        "        model_name=\"gemini-2.0-flash-001\",\n",
        "        generation_config=generation_config,\n",
        "        system_instruction=system_instruction,\n",
        "    )\n",
        "\n",
        "    # Start chat and send user query\n",
        "    chat_session = model.start_chat(history=[])\n",
        "    response = chat_session.send_message(user_query)\n",
        "\n",
        "    # Parse the response\n",
        "    try:\n",
        "        result = json.loads(response.text)\n",
        "        return result\n",
        "    except json.JSONDecodeError:\n",
        "        # Fallback if response is not valid JSON\n",
        "        return {\n",
        "            \"needs_search\": True,\n",
        "            \"reason\": \"Failed to parse response, defaulting to search required\",\n",
        "            \"atomic_questions\": [user_query]\n",
        "        }\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    query = \"What are the latest developments in AI, And how it have affected the indians?\"\n",
        "    result = process_query(query)\n",
        "    print(json.dumps(result, indent=2))"
      ],
      "metadata": {
        "id": "edHxy_ZJi9js",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "6773ba4f-0aaf-415e-a094-bfc46f63a93b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"needs_search\": true,\n",
            "  \"reason\": \"The query asks about the 'latest developments' which requires up-to-date information and the impact on Indians in 2025.\",\n",
            "  \"atomic_questions\": [\n",
            "    \"What are the latest developments in AI in 2025?\",\n",
            "    \"How have these recent AI developments affected Indians in 2025?\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beautifull UI Thanks to Claude ðŸ™‚"
      ],
      "metadata": {
        "id": "sehulnqkzlqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import concurrent.futures\n",
        "import google.generativeai as genai\n",
        "from duckduckgo_search import DDGS\n",
        "import backoff\n",
        "import time\n",
        "import os\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import ipywidgets as widgets\n",
        "from tqdm.notebook import tqdm\n",
        "import markdown2  # Added for Markdown rendering\n",
        "\n",
        "\n",
        "api_key=\"\"   # Replace with your actual API key\n",
        "\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "# Style definitions for the UI\n",
        "css_style = \"\"\"\n",
        "<style>\n",
        "    .app-container {\n",
        "        font-family: 'Roboto', sans-serif;\n",
        "        max-width: 1000px;\n",
        "        margin: 0 auto;\n",
        "        padding: 20px;\n",
        "        background-color: #f9f9f9;\n",
        "        border-radius: 10px;\n",
        "        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);\n",
        "    }\n",
        "    .header {\n",
        "        text-align: center;\n",
        "        margin-bottom: 20px;\n",
        "        color: #2c3e50;\n",
        "    }\n",
        "    .search-container {\n",
        "        margin-bottom: 20px;\n",
        "    }\n",
        "    .result-container {\n",
        "        background-color: white;\n",
        "        padding: 15px;\n",
        "        border-radius: 8px;\n",
        "        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);\n",
        "        margin-top: 20px;\n",
        "    }\n",
        "    .answer-box {\n",
        "        background-color: #e8f4f8;\n",
        "        padding: 15px;\n",
        "        border-radius: 8px;\n",
        "        border-left: 5px solid #3498db;\n",
        "        margin-top: 10px;\n",
        "    }\n",
        "    /* Added styles for Markdown */\n",
        "    .answer-box h1, .answer-box h2, .answer-box h3 {\n",
        "        color: #2c3e50;\n",
        "        margin-top: 10px;\n",
        "        margin-bottom: 10px;\n",
        "    }\n",
        "    .answer-box ul, .answer-box ol {\n",
        "        margin-left: 20px;\n",
        "    }\n",
        "    .answer-box code {\n",
        "        background-color: #f0f0f0;\n",
        "        padding: 2px 4px;\n",
        "        border-radius: 3px;\n",
        "    }\n",
        "    .answer-box pre {\n",
        "        background-color: #f0f0f0;\n",
        "        padding: 10px;\n",
        "        border-radius: 5px;\n",
        "        overflow-x: auto;\n",
        "    }\n",
        "    .atomic-container {\n",
        "        margin-top: 20px;\n",
        "        padding: 10px;\n",
        "        background-color: #f5f5f5;\n",
        "        border-radius: 8px;\n",
        "    }\n",
        "    .atomic-question {\n",
        "        font-weight: bold;\n",
        "        color: #2980b9;\n",
        "        margin-top: 10px;\n",
        "    }\n",
        "    .summary-item {\n",
        "        background-color: white;\n",
        "        padding: 10px;\n",
        "        margin: 8px 0;\n",
        "        border-radius: 6px;\n",
        "        border-left: 3px solid #27ae60;\n",
        "    }\n",
        "    .citation {\n",
        "        font-size: 0.8em;\n",
        "        color: #7f8c8d;\n",
        "        margin-top: 5px;\n",
        "    }\n",
        "    .progress-container {\n",
        "        margin-top: 20px;\n",
        "        text-align: center;\n",
        "    }\n",
        "    .status-message {\n",
        "        margin: 15px 0;\n",
        "        color: #2c3e50;\n",
        "        font-style: italic;\n",
        "    }\n",
        "    .debug-info {\n",
        "        font-family: monospace;\n",
        "        font-size: 0.8em;\n",
        "        background-color: #f0f0f0;\n",
        "        padding: 10px;\n",
        "        border-radius: 5px;\n",
        "        margin-top: 20px;\n",
        "        display: none;\n",
        "    }\n",
        "    .loader {\n",
        "        display: inline-block;\n",
        "        width: 30px;\n",
        "        height: 30px;\n",
        "        border: 3px solid rgba(0,0,0,.3);\n",
        "        border-radius: 50%;\n",
        "        border-top-color: #3498db;\n",
        "        animation: spin 1s ease-in-out infinite;\n",
        "    }\n",
        "    @keyframes spin {\n",
        "        to { transform: rotate(360deg); }\n",
        "    }\n",
        "    .error-message {\n",
        "        color: #e74c3c;\n",
        "        padding: 10px;\n",
        "        background-color: #fadbd8;\n",
        "        border-radius: 5px;\n",
        "        margin: 10px 0;\n",
        "    }\n",
        "</style>\n",
        "\"\"\"\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Core functionality (from your original code, unchanged)\n",
        "# ---------------------------------------------------------------------------\n",
        "def get_full_content(url):\n",
        "    \"\"\"Fetch the full content of a webpage\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        paragraphs = soup.find_all('p')\n",
        "        full_text = ' '.join([p.get_text() for p in paragraphs])\n",
        "        return full_text\n",
        "    except Exception as e:\n",
        "        return f\"Error fetching content: {str(e)}\"\n",
        "\n",
        "def search_with_full_content(query, max_results=2):\n",
        "    \"\"\"Search and retrieve full content for each result\"\"\"\n",
        "    ddgs = DDGS()\n",
        "    results = ddgs.text(keywords=query, max_results=max_results)\n",
        "    enhanced_results = []\n",
        "    for result in results:\n",
        "        # Get the full content for each search result using the href.\n",
        "        result['full_content'] = get_full_content(result['href'])\n",
        "        enhanced_results.append(result)\n",
        "    return enhanced_results\n",
        "\n",
        "@backoff.on_exception(backoff.expo, (requests.exceptions.RequestException, ConnectionError), max_tries=3)\n",
        "def flash_answer(prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Calls the flash model with the provided prompt.\n",
        "    \"\"\"\n",
        "    system_instruction = \"Answer the below question \"\n",
        "    generation_config = {\n",
        "        \"temperature\": 0.2,\n",
        "        \"top_p\": 0.95,\n",
        "        \"top_k\": 40,\n",
        "        \"max_output_tokens\": 8192,\n",
        "    }\n",
        "    flash_model = genai.GenerativeModel(\n",
        "        model_name=\"gemini-2.0-flash-001\",\n",
        "        generation_config=generation_config,\n",
        "        system_instruction=system_instruction,\n",
        "    )\n",
        "    chat_session = flash_model.start_chat(history=[])\n",
        "    response = chat_session.send_message(prompt)\n",
        "    return response.text.strip()\n",
        "\n",
        "def flash_answer_(prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Calls the flash model with the provided prompt.\n",
        "    Adjust system_instruction as needed.\n",
        "    \"\"\"\n",
        "    system_instruction = \"\"\"You are a helpful agent with access to internet search results with proper citation.\n",
        "     Answer the Query using those results with proper citation. Format your answer in Markdown.\"\"\"\n",
        "    generation_config = {\n",
        "        \"temperature\": 0.2,\n",
        "        \"top_p\": 0.95,\n",
        "        \"top_k\": 40,\n",
        "        \"max_output_tokens\": 8192,\n",
        "    }\n",
        "    flash_model = genai.GenerativeModel(\n",
        "        model_name=\"gemini-2.0-flash-001\",\n",
        "        generation_config=generation_config,\n",
        "        system_instruction=system_instruction,\n",
        "    )\n",
        "    chat_session = flash_model.start_chat(history=[])\n",
        "    response = chat_session.send_message(prompt)\n",
        "    return response.text.strip()\n",
        "\n",
        "def summarize_result(result: dict, atomic_question: str) -> dict:\n",
        "    \"\"\"\n",
        "    Given a search result with full_content, send a prompt to Flash model\n",
        "    to summarize the full content in 20-50 words in the context of the atomic question.\n",
        "    \"\"\"\n",
        "    href = result.get('href', '')\n",
        "    full_content = result.get('full_content', '')\n",
        "\n",
        "    prompt = (\n",
        "        f\"Summarize the following content in 30-60 words, focusing on answering the question: '{atomic_question}'.\\n\\n\"\n",
        "        f\"Content: {full_content}\\n\\n\"\n",
        "        f\"Include the citation (URL) at the end{href}.\"\n",
        "    )\n",
        "    summary = flash_answer(prompt)\n",
        "    # Store the summary along with the citation (href) and title.\n",
        "    return {\n",
        "        \"title\": result.get(\"title\", \"\"),\n",
        "        \"href\": href,\n",
        "        \"summary\": summary\n",
        "    }\n",
        "\n",
        "def process_atomic_question(atomic_question: str, progress_callback=None) -> dict:\n",
        "    \"\"\"\n",
        "    For a given atomic question, perform a DuckDuckGo search (with full content)\n",
        "    and concurrently summarize each search result.\n",
        "    \"\"\"\n",
        "    if progress_callback:\n",
        "        progress_callback(f\"Searching for: {atomic_question}\")\n",
        "\n",
        "    # Here we request two search results per atomic question.\n",
        "    search_results = search_with_full_content(atomic_question, max_results=2)\n",
        "\n",
        "    if progress_callback:\n",
        "        progress_callback(f\"Found {len(search_results)} results for: {atomic_question}\")\n",
        "\n",
        "    summaries = []\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        futures = [\n",
        "            executor.submit(summarize_result, result, atomic_question)\n",
        "            for result in search_results\n",
        "        ]\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            summarized = future.result()\n",
        "            summaries.append(summarized)\n",
        "            if progress_callback:\n",
        "                progress_callback(f\"Summarized a result for: {atomic_question}\")\n",
        "\n",
        "    return {atomic_question: summaries}\n",
        "\n",
        "def process_all_atomic_questions(atomic_questions: list, progress_callback=None) -> dict:\n",
        "    \"\"\"\n",
        "    Process all atomic questions concurrently and return a mapping from each atomic\n",
        "    question to its list of summarized search results.\n",
        "    \"\"\"\n",
        "    atomic_summaries = {}\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        future_to_atomic = {\n",
        "            executor.submit(process_atomic_question, aq, progress_callback): aq\n",
        "            for aq in atomic_questions\n",
        "        }\n",
        "        for future in concurrent.futures.as_completed(future_to_atomic):\n",
        "            atomic = future_to_atomic[future]\n",
        "            atomic_summaries.update(future.result())\n",
        "    return atomic_summaries\n",
        "\n",
        "def final_answer(original_query: str, atomic_summaries: dict) -> str:\n",
        "    \"\"\"\n",
        "    Construct a prompt that provides all the summarized content (with citations)\n",
        "    from each atomic question and asks the model to generate a final answer\n",
        "    to the original query.\n",
        "    \"\"\"\n",
        "    prompt = \"Using the following summarized search results (with citations), answer the original query. \" \\\n",
        "             \"Make sure to include proper citations for each piece of information.\\n\\n\" \\\n",
        "             \"Format your answer using Markdown syntax with proper headings, lists, and citation links. \" \\\n",
        "             \"Answer followed by citation.\"\n",
        "\n",
        "    for atomic, summaries in atomic_summaries.items():\n",
        "        prompt += f\"\\nAtomic Question: {atomic}\\n\"\n",
        "        for item in summaries:\n",
        "            summary = item.get('summary', '')\n",
        "            href = item.get('href', '')\n",
        "            prompt += f\"- Summary: {summary}\\n\"\n",
        "            prompt += f\"  Citation: {href}\\n\"\n",
        "        prompt += \"\\n\"\n",
        "\n",
        "    prompt += f\"Original Query: {original_query}\"\n",
        "    return flash_answer_(prompt)\n",
        "\n",
        "def process_query(user_query):\n",
        "    \"\"\"\n",
        "    Determines if a query requires search and breaks it into atomic questions if needed.\n",
        "    \"\"\"\n",
        "    system_instruction = \"\"\"\n",
        "    Today Date = 29-03-2025\n",
        "    You are an assistant that determines if a query requires internet search. Analyze the query and return a JSON with:\n",
        "    1. \"needs_search\": boolean - true if the query requires recent information (after November 2023) or specific facts\n",
        "    2. \"reason\": string - brief explanation why search is/isn't needed\n",
        "    3. \"atomic_questions\": array - if query is complex, break it into smaller atomic questions (empty if search not needed)\n",
        "\n",
        "    IMPORTANT: Any query about events, products, news, or data after November 2023 MUST have \"needs_search\" set to true.\n",
        "    If you are not familiar with the term asked in the question then also turn \"needs_search\" to true.\n",
        "    For small query return the original query. For complex and long queries requiring search, break them down into simpler 2-3 sub-questions.\n",
        "    Whenever someone asks about recent add 2025 in the sub questions.\n",
        "    \"\"\"\n",
        "\n",
        "    generation_config = {\n",
        "        \"temperature\": 0.2,\n",
        "        \"top_p\": 0.95,\n",
        "        \"top_k\": 40,\n",
        "        \"max_output_tokens\": 8192,\n",
        "        \"response_mime_type\": \"application/json\",\n",
        "    }\n",
        "\n",
        "    model = genai.GenerativeModel(\n",
        "        model_name=\"gemini-2.0-flash-001\",\n",
        "        generation_config=generation_config,\n",
        "        system_instruction=system_instruction,\n",
        "    )\n",
        "\n",
        "    chat_session = model.start_chat(history=[])\n",
        "    response = chat_session.send_message(user_query)\n",
        "\n",
        "    try:\n",
        "        result = json.loads(response.text)\n",
        "        return result\n",
        "    except json.JSONDecodeError:\n",
        "        # Fallback if response is not valid JSON\n",
        "        return {\n",
        "            \"needs_search\": True,\n",
        "            \"reason\": \"Failed to parse response, defaulting to search required\",\n",
        "            \"atomic_questions\": [user_query]\n",
        "        }\n",
        "\n",
        "def process_with_full_search(original_query: str, process_query_result: dict, progress_callback=None) -> dict:\n",
        "    \"\"\"\n",
        "    If search is not required (needs_search is false), directly answer the query\n",
        "    Otherwise, for each atomic sub-question, perform full search with content extraction and summarization,\n",
        "    then combine the summaries to answer the original query with citations.\n",
        "    \"\"\"\n",
        "    if not process_query_result.get(\"needs_search\", True):\n",
        "        if progress_callback:\n",
        "            progress_callback(\"Direct answer (no search needed)\")\n",
        "        answer = flash_answer(original_query)\n",
        "        return {\"answer\": answer}\n",
        "    else:\n",
        "        atomic_questions = process_query_result.get(\"atomic_questions\", [])\n",
        "        if progress_callback:\n",
        "            progress_callback(f\"Processing {len(atomic_questions)} atomic questions\")\n",
        "\n",
        "        atomic_summaries = process_all_atomic_questions(atomic_questions, progress_callback)\n",
        "\n",
        "        if progress_callback:\n",
        "            progress_callback(\"Generating final answer\")\n",
        "\n",
        "        final_ans = final_answer(original_query, atomic_summaries)\n",
        "        return {\"answer\": final_ans, \"atomic_summaries\": atomic_summaries}\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Enhanced UI Components with Markdown Support\n",
        "# ---------------------------------------------------------------------------\n",
        "def create_search_ui():\n",
        "    \"\"\"Create and display the search interface\"\"\"\n",
        "    display(HTML(css_style))\n",
        "\n",
        "    # App container\n",
        "    app_html = \"\"\"\n",
        "    <div class=\"app-container\">\n",
        "        <div class=\"header\">\n",
        "            <h1>AI-Powered Web Search</h1>\n",
        "            <p>Ask any question to get researched answers with citations</p>\n",
        "        </div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    display(HTML(app_html))\n",
        "\n",
        "    # Create widgets\n",
        "    query_input = widgets.Text(\n",
        "        description='Query:',\n",
        "        placeholder='Enter your question here...',\n",
        "        layout=widgets.Layout(width='80%')\n",
        "    )\n",
        "\n",
        "    search_button = widgets.Button(\n",
        "        description='Search',\n",
        "        button_style='primary',\n",
        "        icon='search'\n",
        "    )\n",
        "\n",
        "    debug_checkbox = widgets.Checkbox(\n",
        "        value=False,\n",
        "        description='Show debug info',\n",
        "        layout=widgets.Layout(width='auto')\n",
        "    )\n",
        "\n",
        "    output_area = widgets.Output()\n",
        "    status_area = widgets.Output()\n",
        "\n",
        "    # Display widgets\n",
        "    display(widgets.HBox([query_input, search_button]))\n",
        "    display(debug_checkbox)\n",
        "    display(status_area)\n",
        "    display(output_area)\n",
        "\n",
        "    # Progress updates\n",
        "    def update_status(message):\n",
        "        with status_area:\n",
        "            clear_output(wait=True)\n",
        "            status_html = f'<div class=\"status-message\"><div class=\"loader\"></div> {message}</div>'\n",
        "            display(HTML(status_html))\n",
        "\n",
        "    # Handle search button click\n",
        "    def on_search_button_clicked(b):\n",
        "        query = query_input.value.strip()\n",
        "        if not query:\n",
        "            with output_area:\n",
        "                clear_output(wait=True)\n",
        "                display(HTML('<div class=\"error-message\">Please enter a query</div>'))\n",
        "            return\n",
        "\n",
        "        with output_area:\n",
        "            clear_output(wait=True)\n",
        "\n",
        "        # Process the query\n",
        "        update_status(\"Analyzing your query...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            query_analysis = process_query(query)\n",
        "            show_debug = debug_checkbox.value\n",
        "\n",
        "            # Display query analysis if debug is enabled\n",
        "            if show_debug:\n",
        "                with output_area:\n",
        "                    analysis_json = json.dumps(query_analysis, indent=2)\n",
        "                    debug_html = f\"\"\"\n",
        "                    <div class=\"debug-info\" style=\"display: block;\">\n",
        "                        <h3>Query Analysis:</h3>\n",
        "                        <pre>{analysis_json}</pre>\n",
        "                    </div>\n",
        "                    \"\"\"\n",
        "                    display(HTML(debug_html))\n",
        "\n",
        "            if not query_analysis.get(\"needs_search\", True):\n",
        "                update_status(\"Generating direct answer (no search needed)...\")\n",
        "                result = process_with_full_search(query, query_analysis, update_status)\n",
        "\n",
        "                with output_area:\n",
        "                    # Convert Markdown to HTML\n",
        "                    answer_html = markdown2.markdown(result['answer'])\n",
        "                    time_taken = time.time() - start_time\n",
        "                    direct_html = f\"\"\"\n",
        "                    <div class=\"result-container\">\n",
        "                        <h2>Answer</h2>\n",
        "                        <div class=\"answer-box\">{answer_html}</div>\n",
        "                        <p><em>Answered directly without search (took {time_taken:.2f} seconds)</em></p>\n",
        "                    </div>\n",
        "                    \"\"\"\n",
        "                    display(HTML(direct_html))\n",
        "            else:\n",
        "                atomic_questions = query_analysis.get(\"atomic_questions\", [])\n",
        "                update_status(f\"Breaking down into {len(atomic_questions)} sub-questions...\")\n",
        "\n",
        "                result = process_with_full_search(query, query_analysis, update_status)\n",
        "\n",
        "                # Convert Markdown to HTML\n",
        "                answer_html_content = markdown2.markdown(result['answer'])\n",
        "                time_taken = time.time() - start_time\n",
        "\n",
        "                # Build the HTML in parts to avoid nested f-string issues\n",
        "                answer_html = f\"\"\"\n",
        "                <div class=\"result-container\">\n",
        "                    <h2>Answer</h2>\n",
        "                    <div class=\"answer-box\">{answer_html_content}</div>\n",
        "                \"\"\"\n",
        "\n",
        "                if show_debug and \"atomic_summaries\" in result:\n",
        "                    answer_html += \"\"\"\n",
        "                    <div class=\"atomic-container\">\n",
        "                        <h3>Research Process:</h3>\n",
        "                    \"\"\"\n",
        "\n",
        "                    for question, summaries in result[\"atomic_summaries\"].items():\n",
        "                        question_html = f\"\"\"\n",
        "                        <div class=\"atomic-question\">{question}</div>\n",
        "                        \"\"\"\n",
        "                        answer_html += question_html\n",
        "\n",
        "                        for summary in summaries:\n",
        "                            summary_text = summary['summary'].replace('\\n', '<br>')\n",
        "                            summary_href = summary['href']\n",
        "                            summary_title = summary['title'] or summary['href']\n",
        "\n",
        "                            summary_html = f\"\"\"\n",
        "                            <div class=\"summary-item\">\n",
        "                                <div>{summary_text}</div>\n",
        "                                <div class=\"citation\">Source: <a href=\"{summary_href}\" target=\"_blank\">{summary_title}</a></div>\n",
        "                            </div>\n",
        "                            \"\"\"\n",
        "                            answer_html += summary_html\n",
        "\n",
        "                    answer_html += \"</div>\"\n",
        "\n",
        "                footer_html = f\"\"\"\n",
        "                    <p><em>Researched answer (took {time_taken:.2f} seconds)</em></p>\n",
        "                </div>\n",
        "                \"\"\"\n",
        "                answer_html += footer_html\n",
        "\n",
        "                with output_area:\n",
        "                    display(HTML(answer_html))\n",
        "\n",
        "        except Exception as e:\n",
        "            with output_area:\n",
        "                error_message = str(e)\n",
        "                error_html = f\"\"\"\n",
        "                <div class=\"error-message\">\n",
        "                    <h3>Error occurred:</h3>\n",
        "                    <p>{error_message}</p>\n",
        "                </div>\n",
        "                \"\"\"\n",
        "                display(HTML(error_html))\n",
        "\n",
        "        with status_area:\n",
        "            clear_output()\n",
        "\n",
        "    search_button.on_click(on_search_button_clicked)\n",
        "\n",
        "    # Handle Enter key in query input\n",
        "    def on_enter(widget):\n",
        "        on_search_button_clicked(None)\n",
        "\n",
        "    query_input.on_submit(on_enter)\n",
        "\n",
        "    return {\n",
        "        'query_input': query_input,\n",
        "        'search_button': search_button,\n",
        "        'output_area': output_area,\n",
        "        'status_area': status_area\n",
        "    }\n",
        "\n",
        "# Function to run the app\n",
        "def run_search_app():\n",
        "    \"\"\"Main function to run the search application\"\"\"\n",
        "    # Check if API key is set\n",
        "    if not api_key or api_key == \"YOUR_API_KEY_HERE\":\n",
        "        api_key_warning = \"\"\"\n",
        "        <div style=\"color: red; padding: 10px; background-color: #ffe6e6; border-radius: 5px; margin: 10px 0;\">\n",
        "            <h3>âš ï¸ API Key Missing</h3>\n",
        "            <p>Please set your Google AI API key in the code before running.</p>\n",
        "            <code>api_key = \"your_actual_api_key_here\"</code>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "        display(HTML(api_key_warning))\n",
        "        return\n",
        "\n",
        "    # Create and display UI\n",
        "    ui_components = create_search_ui()\n",
        "\n",
        "    print(\"ðŸ‘† Search app is ready to use!\")\n",
        "\n",
        "# Run the app when executed\n",
        "if __name__ == \"__main__\" or 'google.colab' in str(get_ipython()):\n",
        "    run_search_app()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622,
          "referenced_widgets": [
            "0cb4d957f1c84520bb89d58823797c8a",
            "0a4e191701c64005b72553cde9a6861e",
            "e2f07c0962a34068bfedc2a24fb57a5a",
            "2ac6eb54d3c04090ae102a14b3b8889b",
            "3df0c98f71da4757801269400be3c504",
            "633ad8f8a5da4c4496dd97009cc6b5b7",
            "cd27f83537c3414495f819b386f5cf7b",
            "bffb385eacc844ceab3267001fb403e2",
            "9316c7f7741848a28909d5a5f2c08c4a",
            "33a7bc344b8540d0b1d719e99320d8cc",
            "0483e62f41ec459bb7a85751848b8b17",
            "e831c48a37014a1e915d3ed7c68e68e7",
            "d1b5e6e3d91f49af9a62c727889d8eb0",
            "377a9c92b6f1421faa4da00d4d58bf15",
            "f4b543382e7c4483b57c354663ae2ed7"
          ]
        },
        "id": "RlaC9VoEkey3",
        "outputId": "d6df99b4-8ec9-4046-afb7-9f97bc9e6e4e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    .app-container {\n",
              "        font-family: 'Roboto', sans-serif;\n",
              "        max-width: 1000px;\n",
              "        margin: 0 auto;\n",
              "        padding: 20px;\n",
              "        background-color: #f9f9f9;\n",
              "        border-radius: 10px;\n",
              "        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);\n",
              "    }\n",
              "    .header {\n",
              "        text-align: center;\n",
              "        margin-bottom: 20px;\n",
              "        color: #2c3e50;\n",
              "    }\n",
              "    .search-container {\n",
              "        margin-bottom: 20px;\n",
              "    }\n",
              "    .result-container {\n",
              "        background-color: white;\n",
              "        padding: 15px;\n",
              "        border-radius: 8px;\n",
              "        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);\n",
              "        margin-top: 20px;\n",
              "    }\n",
              "    .answer-box {\n",
              "        background-color: #e8f4f8;\n",
              "        padding: 15px;\n",
              "        border-radius: 8px;\n",
              "        border-left: 5px solid #3498db;\n",
              "        margin-top: 10px;\n",
              "    }\n",
              "    /* Added styles for Markdown */\n",
              "    .answer-box h1, .answer-box h2, .answer-box h3 {\n",
              "        color: #2c3e50;\n",
              "        margin-top: 10px;\n",
              "        margin-bottom: 10px;\n",
              "    }\n",
              "    .answer-box ul, .answer-box ol {\n",
              "        margin-left: 20px;\n",
              "    }\n",
              "    .answer-box code {\n",
              "        background-color: #f0f0f0;\n",
              "        padding: 2px 4px;\n",
              "        border-radius: 3px;\n",
              "    }\n",
              "    .answer-box pre {\n",
              "        background-color: #f0f0f0;\n",
              "        padding: 10px;\n",
              "        border-radius: 5px;\n",
              "        overflow-x: auto;\n",
              "    }\n",
              "    .atomic-container {\n",
              "        margin-top: 20px;\n",
              "        padding: 10px;\n",
              "        background-color: #f5f5f5;\n",
              "        border-radius: 8px;\n",
              "    }\n",
              "    .atomic-question {\n",
              "        font-weight: bold;\n",
              "        color: #2980b9;\n",
              "        margin-top: 10px;\n",
              "    }\n",
              "    .summary-item {\n",
              "        background-color: white;\n",
              "        padding: 10px;\n",
              "        margin: 8px 0;\n",
              "        border-radius: 6px;\n",
              "        border-left: 3px solid #27ae60;\n",
              "    }\n",
              "    .citation {\n",
              "        font-size: 0.8em;\n",
              "        color: #7f8c8d;\n",
              "        margin-top: 5px;\n",
              "    }\n",
              "    .progress-container {\n",
              "        margin-top: 20px;\n",
              "        text-align: center;\n",
              "    }\n",
              "    .status-message {\n",
              "        margin: 15px 0;\n",
              "        color: #2c3e50;\n",
              "        font-style: italic;\n",
              "    }\n",
              "    .debug-info {\n",
              "        font-family: monospace;\n",
              "        font-size: 0.8em;\n",
              "        background-color: #f0f0f0;\n",
              "        padding: 10px;\n",
              "        border-radius: 5px;\n",
              "        margin-top: 20px;\n",
              "        display: none;\n",
              "    }\n",
              "    .loader {\n",
              "        display: inline-block;\n",
              "        width: 30px;\n",
              "        height: 30px;\n",
              "        border: 3px solid rgba(0,0,0,.3);\n",
              "        border-radius: 50%;\n",
              "        border-top-color: #3498db;\n",
              "        animation: spin 1s ease-in-out infinite;\n",
              "    }\n",
              "    @keyframes spin {\n",
              "        to { transform: rotate(360deg); }\n",
              "    }\n",
              "    .error-message {\n",
              "        color: #e74c3c;\n",
              "        padding: 10px;\n",
              "        background-color: #fadbd8;\n",
              "        border-radius: 5px;\n",
              "        margin: 10px 0;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div class=\"app-container\">\n",
              "        <div class=\"header\">\n",
              "            <h1>AI-Powered Web Search</h1>\n",
              "            <p>Ask any question to get researched answers with citations</p>\n",
              "        </div>\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(Text(value='', description='Query:', layout=Layout(width='80%'), placeholder='Enter your questiâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0cb4d957f1c84520bb89d58823797c8a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Checkbox(value=False, description='Show debug info', layout=Layout(width='auto'))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9316c7f7741848a28909d5a5f2c08c4a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e831c48a37014a1e915d3ed7c68e68e7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "377a9c92b6f1421faa4da00d4d58bf15"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ‘† Search app is ready to use!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:backoff:Backing off flash_answer(...) for 0.9s (requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Not needed"
      ],
      "metadata": {
        "id": "yJ84W9Z_zLfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query search using duckduck go\n",
        "\n",
        "Will try this reranker later\n",
        "\n",
        "[Reranker](https://app.contextual.ai/)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lZCKlORub5KR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import google.generativeai as genai\n",
        "\n",
        "def flash_answer(query: str) -> str:\n",
        "    system_instruction = \"Answer the Below question\"\n",
        "    generation_config = {\n",
        "        \"temperature\": 0.2,\n",
        "        \"top_p\": 0.95,\n",
        "        \"top_k\": 40,\n",
        "        \"max_output_tokens\": 8192,\n",
        "    }\n",
        "\n",
        "    # Create the model\n",
        "    flash_model = genai.GenerativeModel(\n",
        "        model_name=\"gemini-2.0-flash-001\",\n",
        "        generation_config=generation_config,\n",
        "        system_instruction=system_instruction,\n",
        "    )\n",
        "\n",
        "    chat_session = flash_model.start_chat(history=[])\n",
        "    response = chat_session.send_message(query)\n",
        "    return response.text\n",
        "\n",
        "# Provided simple DuckDuckGo search function wrapper.\n",
        "def search_ddg(\n",
        "    query: str,\n",
        "    search_type: str = \"search\",\n",
        "    max_results: int = 5,\n",
        "    timeout: int = 10\n",
        ") -> list:\n",
        "    try:\n",
        "        from duckduckgo_search import DDGS\n",
        "    except ImportError:\n",
        "        print(\"Error: 'duckduckgo-search' package not installed!\")\n",
        "        print(\"Install it with: pip install duckduckgo-search\")\n",
        "        return []\n",
        "    ddgs = DDGS(timeout=timeout)\n",
        "    if search_type.lower() == \"news\":\n",
        "        return ddgs.news(keywords=query, max_results=max_results)\n",
        "    else:\n",
        "        return ddgs.text(keywords=query, max_results=max_results)\n",
        "\n",
        "def simple_ddg_search(\n",
        "    query: str,\n",
        "    search_type: str = \"search\",\n",
        "    max_results: int = 2,\n",
        "    return_json: bool = False\n",
        "):\n",
        "    results = search_ddg(query, search_type, max_results)\n",
        "    if return_json:\n",
        "        return json.dumps(results, indent=2)\n",
        "    return results\n",
        "\n",
        "# Main function that uses the result from process_query to decide the next step.\n",
        "def process_result(result: dict, user_query: str):\n",
        "    \"\"\"\n",
        "    If search is not needed, answer the user's original query using flash 1.5.\n",
        "    Otherwise, call simple_ddg_search on each atomic question and return a dictionary\n",
        "    where keys are atomic questions and values are the search results.\n",
        "    \"\"\"\n",
        "    if not result.get(\"needs_search\", True):\n",
        "        # No search required; answer directly with flash 1.5.\n",
        "        answer = flash_answer(user_query)\n",
        "        return {\"answer\": answer}\n",
        "    else:\n",
        "        # Search is needed; iterate over atomic questions.\n",
        "        search_results = {}\n",
        "        for atomic in result.get(\"atomic_questions\", []):\n",
        "            # Perform DDG search for each atomic sub-question.\n",
        "            atomic_result = simple_ddg_search(query=atomic)\n",
        "            search_results[atomic] = atomic_result\n",
        "        return search_results\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "  # Example original query.\n",
        "    query = \"What are the latest developments in AI, And how it have affected the indians?\"\n",
        "    process_query_output = process_query(query)\n",
        "    print(process_query_output)\n",
        "    final_output = process_result(process_query_output, query)\n",
        "    print(json.dumps(final_output, indent=2))\n"
      ],
      "metadata": {
        "id": "B3U2TANmoLvu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "outputId": "7f1de2a6-90e2-4b5c-a9b3-b5210614f1e0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'needs_search': True, 'reason': \"The query asks about 'latest developments' which requires up-to-date information and the effect of AI on Indians in 2025.\", 'atomic_questions': ['What are the latest developments in AI in 2025?', 'How have the latest AI developments affected Indians in 2025?']}\n",
            "{\n",
            "  \"What are the latest developments in AI in 2025?\": [\n",
            "    {\n",
            "      \"title\": \"The 10 Biggest AI Trends Of 2025 Everyone Must Be Ready For Today - Forbes\",\n",
            "      \"href\": \"https://www.forbes.com/sites/bernardmarr/2024/09/24/the-10-biggest-ai-trends-of-2025-everyone-must-be-ready-for-today/\",\n",
            "      \"body\": \"Discover the 10 major AI trends set to reshape 2025: from augmented working and real-time decision-making to advanced AI legislation and sustainable AI initiatives.\"\n",
            "    },\n",
            "    {\n",
            "      \"title\": \"Future of AI in 2025 [Top Trends and Predictions] - GeeksforGeeks\",\n",
            "      \"href\": \"https://www.geeksforgeeks.org/future-of-ai/\",\n",
            "      \"body\": \"Artificial Intelligence (AI) is changing how industries work and bringing new ways to innovate. As we get closer to 2025, AI's impact will grow even more, with exciting new developments happening in different areas. This article explores the Top 10 Artificial Intelligence(AI) Predictions for 2025, w\"\n",
            "    }\n",
            "  ],\n",
            "  \"How have the latest AI developments affected Indians in 2025?\": [\n",
            "    {\n",
            "      \"title\": \"What Does the Future Hold for the Development of AI in India?\",\n",
            "      \"href\": \"https://thediplomat.com/2025/03/what-does-the-future-hold-for-the-development-of-ai-in-india/\",\n",
            "      \"body\": \"For example, only half a percent of the patents in the world's AI industry were granted to India between 2010 and 2022, and India's state-funded AI program is worth only $1 billion. Yet, India ...\"\n",
            "    },\n",
            "    {\n",
            "      \"title\": \"AI in India: Trends, Opportunities, and Challenges in 2025\",\n",
            "      \"href\": \"https://www.analyticsinsight.net/artificial-intelligence/ai-in-india-trends-opportunities-and-challenges-in-2025\",\n",
            "      \"body\": \"With the right policies, investments, and partnerships, AI can drive economic growth, create new job opportunities, and solve pressing social challenges. The goal should be to build an AI-powered future that benefits everyone. The journey won't be easy. But with a clear strategy and collaborative efforts, India can harness AI's full ...\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install backoff markdown2"
      ],
      "metadata": {
        "id": "vshUk3ix5nPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import concurrent.futures\n",
        "import google.generativeai as genai\n",
        "from duckduckgo_search import DDGS\n",
        "import backoff\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Provided functions for fetching full content and DuckDuckGo search\n",
        "# ---------------------------------------------------------------------------\n",
        "def get_full_content(url):\n",
        "    \"\"\"Fetch the full content of a webpage\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        paragraphs = soup.find_all('p')\n",
        "        full_text = ' '.join([p.get_text() for p in paragraphs])\n",
        "        return full_text\n",
        "    except Exception as e:\n",
        "        return f\"Error fetching content: {str(e)}\"\n",
        "\n",
        "def search_with_full_content(query, max_results=2):\n",
        "    \"\"\"Search and retrieve full content for each result\"\"\"\n",
        "    ddgs = DDGS()\n",
        "    results = ddgs.text(keywords=query, max_results=max_results)\n",
        "    enhanced_results = []\n",
        "    for result in results:\n",
        "        # Get the full content for each search result using the href.\n",
        "        result['full_content'] = get_full_content(result['href'])\n",
        "        enhanced_results.append(result)\n",
        "    return enhanced_results\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Flash 1.5 helper functions\n",
        "# ---------------------------------------------------------------------------\n",
        "@backoff.on_exception(backoff.expo, (requests.exceptions.RequestException, ConnectionError), max_tries=3)  # Apply backoff decorator\n",
        "\n",
        "def flash_answer(prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Calls the flash 1.5 model with the provided prompt.\n",
        "    Adjust system_instruction as needed.\n",
        "    \"\"\"\n",
        "    system_instruction = \"Answer the below question \"\n",
        "    generation_config = {\n",
        "        \"temperature\": 0.2,\n",
        "        \"top_p\": 0.95,\n",
        "        \"top_k\": 40,\n",
        "        \"max_output_tokens\": 8192,\n",
        "    }\n",
        "    flash_model = genai.GenerativeModel(\n",
        "        model_name=\"gemini-2.0-flash-001\",\n",
        "        generation_config=generation_config,\n",
        "        system_instruction=system_instruction,\n",
        "    )\n",
        "    chat_session = flash_model.start_chat(history=[])\n",
        "    response = chat_session.send_message(prompt)\n",
        "    return response.text.strip()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def flash_answer_(prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Calls the flash 1.5 model with the provided prompt.\n",
        "    Adjust system_instruction as needed.\n",
        "    \"\"\"\n",
        "    system_instruction = \"\"\"You are an helpful agent which have access to internet also you will be provided search result with proper citation.\n",
        "     Answer the Query Using those result with proper citation.\"\"\"\n",
        "    generation_config = {\n",
        "        \"temperature\": 0.2,\n",
        "        \"top_p\": 0.95,\n",
        "        \"top_k\": 40,\n",
        "        \"max_output_tokens\": 8192,\n",
        "    }\n",
        "    flash_model = genai.GenerativeModel(\n",
        "        model_name=\"gemini-2.0-flash-001\",\n",
        "        generation_config=generation_config,\n",
        "        system_instruction=system_instruction,\n",
        "    )\n",
        "    chat_session = flash_model.start_chat(history=[])\n",
        "    response = chat_session.send_message(prompt)\n",
        "    return response.text.strip()\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Concurrent summarization for each atomic sub-question\n",
        "# ---------------------------------------------------------------------------\n",
        "def summarize_result(result: dict, atomic_question: str) -> dict:\n",
        "    \"\"\"\n",
        "    Given a search result with full_content, send a prompt to Flash 1.5\n",
        "    to summarize the full content in 20-50 words in the context of the atomic question.\n",
        "    \"\"\"\n",
        "    prompt = (\n",
        "        f\"Summarize the following content in 30-60 words, focusing on answering the question: '{atomic_question}'.\\n\\n\"\n",
        "        f\"Content: {result.get('full_content', '')}\\n\\n\"\n",
        "        f\"Include the citation (URL) at the end{result.get('href', '')}.\"\n",
        "    )\n",
        "    summary = flash_answer(prompt)\n",
        "    # Store the summary along with the citation (href) and title.\n",
        "    return {\n",
        "        \"title\": result.get(\"title\", \"\"),\n",
        "        \"href\": result.get(\"href\", \"\"),\n",
        "        \"summary\": summary\n",
        "    }\n",
        "\n",
        "def process_atomic_question(atomic_question: str) -> dict:\n",
        "    \"\"\"\n",
        "    For a given atomic question, perform a DuckDuckGo search (with full content)\n",
        "    and concurrently summarize each search result.\n",
        "    \"\"\"\n",
        "    # Here we request two search results per atomic question.\n",
        "    search_results = search_with_full_content(atomic_question, max_results=2)\n",
        "    summaries = []\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        futures = [\n",
        "            executor.submit(summarize_result, result, atomic_question)\n",
        "            for result in search_results\n",
        "        ]\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            summarized = future.result()\n",
        "            summaries.append(summarized)\n",
        "    return {atomic_question: summaries}\n",
        "\n",
        "def process_all_atomic_questions(atomic_questions: list) -> dict:\n",
        "    \"\"\"\n",
        "    Process all atomic questions concurrently and return a mapping from each atomic\n",
        "    question to its list of summarized search results.\n",
        "    \"\"\"\n",
        "    atomic_summaries = {}\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        future_to_atomic = {\n",
        "            executor.submit(process_atomic_question, aq): aq\n",
        "            for aq in atomic_questions\n",
        "        }\n",
        "        for future in concurrent.futures.as_completed(future_to_atomic):\n",
        "            atomic = future_to_atomic[future]\n",
        "            atomic_summaries.update(future.result())\n",
        "    return atomic_summaries\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Compose final answer using summarized search results\n",
        "# ---------------------------------------------------------------------------\n",
        "def final_answer(original_query: str, atomic_summaries: dict) -> str:\n",
        "    \"\"\"\n",
        "    Construct a prompt that provides all the summarized content (with citations)\n",
        "    from each atomic question and asks Flash 1.5 to generate a final answer\n",
        "    to the original query.\n",
        "    \"\"\"\n",
        "    prompt = \"Using the following summarized search results (with citations), answer the original query  \" \\\n",
        "             \"Make sure to include proper citations for each piece of information.\\n\\n Give the answer in proper structure. Answer followed by citation\"\n",
        "    for atomic, summaries in atomic_summaries.items():\n",
        "        prompt += f\"Atomic Question: {atomic}\\n\"\n",
        "        for item in summaries:\n",
        "            prompt += f\"- Summary: {item.get('summary')}\\n\"\n",
        "            prompt += f\"  Citation: {item.get('href')}\\n\"\n",
        "        prompt += \"\\n\"\n",
        "    prompt += f\"Original Query: {original_query}\"\n",
        "    print(prompt)\n",
        "    return flash_answer_(prompt)\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Main function to process the output from the initial process_query\n",
        "# ---------------------------------------------------------------------------\n",
        "def process_with_full_search(original_query: str, process_query_result: dict) -> dict:\n",
        "    \"\"\"\n",
        "    If search is not required (needs_search is false), directly answer the query using flash 2.0\n",
        "    Otherwise, for each atomic sub-question, perform full search with content extraction and summarization,\n",
        "    then combine the summaries to answer the original query with citations.\n",
        "    \"\"\"\n",
        "    if not process_query_result.get(\"needs_search\", True):\n",
        "        answer = flash_answer(original_query)\n",
        "        return {\"answer\": answer}\n",
        "    else:\n",
        "        atomic_questions = process_query_result.get(\"atomic_questions\", [])\n",
        "        atomic_summaries = process_all_atomic_questions(atomic_questions)\n",
        "        final_ans = final_answer(original_query, atomic_summaries)\n",
        "        return {\"answer\": final_ans, \"atomic_summaries\": atomic_summaries}\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Example usage\n",
        "# ---------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Example output from process_query\n",
        "\n",
        "    original_query = \"what are the latest news of today which can come in upsc\"\n",
        "    process_query_output = process_query(original_query)\n",
        "\n",
        "    final_output = process_with_full_search(original_query, process_query_output)\n",
        "    print(json.dumps(final_output, indent=2))\n"
      ],
      "metadata": {
        "id": "J6IZo0EUhwnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bT_TFczQVki1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(final_output[\"answer\"])"
      ],
      "metadata": {
        "id": "4OJtPFL5idDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Search with beautiful UI thanks to claude ðŸ˜Ž"
      ],
      "metadata": {
        "id": "MdHdbx8UkHg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install google-generativeai duckduckgo-search beautifulsoup4 backoff ipywidgets requests tqdm ipython-autotime markdown2\n",
        "\n",
        "import json\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import concurrent.futures\n",
        "import google.generativeai as genai\n",
        "from duckduckgo_search import DDGS\n",
        "import backoff\n",
        "import time\n",
        "import os\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import ipywidgets as widgets\n",
        "from tqdm.notebook import tqdm\n",
        "import markdown2  # Added for Markdown rendering\n",
        "\n",
        "  # Replace with your actual API key\n",
        "\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "# Style definitions for the UI\n",
        "css_style = \"\"\"\n",
        "<style>\n",
        "    .app-container {\n",
        "        font-family: 'Roboto', sans-serif;\n",
        "        max-width: 1000px;\n",
        "        margin: 0 auto;\n",
        "        padding: 20px;\n",
        "        background-color: #f9f9f9;\n",
        "        border-radius: 10px;\n",
        "        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);\n",
        "    }\n",
        "    .header {\n",
        "        text-align: center;\n",
        "        margin-bottom: 20px;\n",
        "        color: #2c3e50;\n",
        "    }\n",
        "    .search-container {\n",
        "        margin-bottom: 20px;\n",
        "    }\n",
        "    .result-container {\n",
        "        background-color: white;\n",
        "        padding: 15px;\n",
        "        border-radius: 8px;\n",
        "        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);\n",
        "        margin-top: 20px;\n",
        "    }\n",
        "    .answer-box {\n",
        "        background-color: #e8f4f8;\n",
        "        padding: 15px;\n",
        "        border-radius: 8px;\n",
        "        border-left: 5px solid #3498db;\n",
        "        margin-top: 10px;\n",
        "    }\n",
        "    /* Added styles for Markdown */\n",
        "    .answer-box h1, .answer-box h2, .answer-box h3 {\n",
        "        color: #2c3e50;\n",
        "        margin-top: 10px;\n",
        "        margin-bottom: 10px;\n",
        "    }\n",
        "    .answer-box ul, .answer-box ol {\n",
        "        margin-left: 20px;\n",
        "    }\n",
        "    .answer-box code {\n",
        "        background-color: #f0f0f0;\n",
        "        padding: 2px 4px;\n",
        "        border-radius: 3px;\n",
        "    }\n",
        "    .answer-box pre {\n",
        "        background-color: #f0f0f0;\n",
        "        padding: 10px;\n",
        "        border-radius: 5px;\n",
        "        overflow-x: auto;\n",
        "    }\n",
        "    .atomic-container {\n",
        "        margin-top: 20px;\n",
        "        padding: 10px;\n",
        "        background-color: #f5f5f5;\n",
        "        border-radius: 8px;\n",
        "    }\n",
        "    .atomic-question {\n",
        "        font-weight: bold;\n",
        "        color: #2980b9;\n",
        "        margin-top: 10px;\n",
        "    }\n",
        "    .summary-item {\n",
        "        background-color: white;\n",
        "        padding: 10px;\n",
        "        margin: 8px 0;\n",
        "        border-radius: 6px;\n",
        "        border-left: 3px solid #27ae60;\n",
        "    }\n",
        "    .citation {\n",
        "        font-size: 0.8em;\n",
        "        color: #7f8c8d;\n",
        "        margin-top: 5px;\n",
        "    }\n",
        "    .progress-container {\n",
        "        margin-top: 20px;\n",
        "        text-align: center;\n",
        "    }\n",
        "    .status-message {\n",
        "        margin: 15px 0;\n",
        "        color: #2c3e50;\n",
        "        font-style: italic;\n",
        "    }\n",
        "    .debug-info {\n",
        "        font-family: monospace;\n",
        "        font-size: 0.8em;\n",
        "        background-color: #f0f0f0;\n",
        "        padding: 10px;\n",
        "        border-radius: 5px;\n",
        "        margin-top: 20px;\n",
        "        display: none;\n",
        "    }\n",
        "    .loader {\n",
        "        display: inline-block;\n",
        "        width: 30px;\n",
        "        height: 30px;\n",
        "        border: 3px solid rgba(0,0,0,.3);\n",
        "        border-radius: 50%;\n",
        "        border-top-color: #3498db;\n",
        "        animation: spin 1s ease-in-out infinite;\n",
        "    }\n",
        "    @keyframes spin {\n",
        "        to { transform: rotate(360deg); }\n",
        "    }\n",
        "    .error-message {\n",
        "        color: #e74c3c;\n",
        "        padding: 10px;\n",
        "        background-color: #fadbd8;\n",
        "        border-radius: 5px;\n",
        "        margin: 10px 0;\n",
        "    }\n",
        "</style>\n",
        "\"\"\"\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Core functionality (from your original code, unchanged)\n",
        "# ---------------------------------------------------------------------------\n",
        "def get_full_content(url):\n",
        "    \"\"\"Fetch the full content of a webpage\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        paragraphs = soup.find_all('p')\n",
        "        full_text = ' '.join([p.get_text() for p in paragraphs])\n",
        "        return full_text\n",
        "    except Exception as e:\n",
        "        return f\"Error fetching content: {str(e)}\"\n",
        "\n",
        "def search_with_full_content(query, max_results=2):\n",
        "    \"\"\"Search and retrieve full content for each result\"\"\"\n",
        "    ddgs = DDGS()\n",
        "    results = ddgs.text(keywords=query, max_results=max_results)\n",
        "    enhanced_results = []\n",
        "    for result in results:\n",
        "        # Get the full content for each search result using the href.\n",
        "        result['full_content'] = get_full_content(result['href'])\n",
        "        enhanced_results.append(result)\n",
        "    return enhanced_results\n",
        "\n",
        "@backoff.on_exception(backoff.expo, (requests.exceptions.RequestException, ConnectionError), max_tries=3)\n",
        "def flash_answer(prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Calls the flash model with the provided prompt.\n",
        "    \"\"\"\n",
        "    system_instruction = \"Answer the below question \"\n",
        "    generation_config = {\n",
        "        \"temperature\": 0.2,\n",
        "        \"top_p\": 0.95,\n",
        "        \"top_k\": 40,\n",
        "        \"max_output_tokens\": 8192,\n",
        "    }\n",
        "    flash_model = genai.GenerativeModel(\n",
        "        model_name=\"gemini-2.0-flash-001\",\n",
        "        generation_config=generation_config,\n",
        "        system_instruction=system_instruction,\n",
        "    )\n",
        "    chat_session = flash_model.start_chat(history=[])\n",
        "    response = chat_session.send_message(prompt)\n",
        "    return response.text.strip()\n",
        "\n",
        "def flash_answer_(prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Calls the flash model with the provided prompt.\n",
        "    Adjust system_instruction as needed.\n",
        "    \"\"\"\n",
        "    system_instruction = \"\"\"You are a helpful agent with access to internet search results with proper citation.\n",
        "     Answer the Query using those results with proper citation. Format your answer in Markdown.\"\"\"\n",
        "    generation_config = {\n",
        "        \"temperature\": 0.2,\n",
        "        \"top_p\": 0.95,\n",
        "        \"top_k\": 40,\n",
        "        \"max_output_tokens\": 8192,\n",
        "    }\n",
        "    flash_model = genai.GenerativeModel(\n",
        "        model_name=\"gemini-2.0-flash-001\",\n",
        "        generation_config=generation_config,\n",
        "        system_instruction=system_instruction,\n",
        "    )\n",
        "    chat_session = flash_model.start_chat(history=[])\n",
        "    response = chat_session.send_message(prompt)\n",
        "    return response.text.strip()\n",
        "\n",
        "def summarize_result(result: dict, atomic_question: str) -> dict:\n",
        "    \"\"\"\n",
        "    Given a search result with full_content, send a prompt to Flash model\n",
        "    to summarize the full content in 20-50 words in the context of the atomic question.\n",
        "    \"\"\"\n",
        "    href = result.get('href', '')\n",
        "    full_content = result.get('full_content', '')\n",
        "\n",
        "    prompt = (\n",
        "        f\"Summarize the following content in 30-60 words, focusing on answering the question: '{atomic_question}'.\\n\\n\"\n",
        "        f\"Content: {full_content}\\n\\n\"\n",
        "        f\"Include the citation (URL) at the end{href}.\"\n",
        "    )\n",
        "    summary = flash_answer(prompt)\n",
        "    # Store the summary along with the citation (href) and title.\n",
        "    return {\n",
        "        \"title\": result.get(\"title\", \"\"),\n",
        "        \"href\": href,\n",
        "        \"summary\": summary\n",
        "    }\n",
        "\n",
        "def process_atomic_question(atomic_question: str, progress_callback=None) -> dict:\n",
        "    \"\"\"\n",
        "    For a given atomic question, perform a DuckDuckGo search (with full content)\n",
        "    and concurrently summarize each search result.\n",
        "    \"\"\"\n",
        "    if progress_callback:\n",
        "        progress_callback(f\"Searching for: {atomic_question}\")\n",
        "\n",
        "    # Here we request two search results per atomic question.\n",
        "    search_results = search_with_full_content(atomic_question, max_results=2)\n",
        "\n",
        "    if progress_callback:\n",
        "        progress_callback(f\"Found {len(search_results)} results for: {atomic_question}\")\n",
        "\n",
        "    summaries = []\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        futures = [\n",
        "            executor.submit(summarize_result, result, atomic_question)\n",
        "            for result in search_results\n",
        "        ]\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            summarized = future.result()\n",
        "            summaries.append(summarized)\n",
        "            if progress_callback:\n",
        "                progress_callback(f\"Summarized a result for: {atomic_question}\")\n",
        "\n",
        "    return {atomic_question: summaries}\n",
        "\n",
        "def process_all_atomic_questions(atomic_questions: list, progress_callback=None) -> dict:\n",
        "    \"\"\"\n",
        "    Process all atomic questions concurrently and return a mapping from each atomic\n",
        "    question to its list of summarized search results.\n",
        "    \"\"\"\n",
        "    atomic_summaries = {}\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        future_to_atomic = {\n",
        "            executor.submit(process_atomic_question, aq, progress_callback): aq\n",
        "            for aq in atomic_questions\n",
        "        }\n",
        "        for future in concurrent.futures.as_completed(future_to_atomic):\n",
        "            atomic = future_to_atomic[future]\n",
        "            atomic_summaries.update(future.result())\n",
        "    return atomic_summaries\n",
        "\n",
        "def final_answer(original_query: str, atomic_summaries: dict) -> str:\n",
        "    \"\"\"\n",
        "    Construct a prompt that provides all the summarized content (with citations)\n",
        "    from each atomic question and asks the model to generate a final answer\n",
        "    to the original query.\n",
        "    \"\"\"\n",
        "    prompt = \"Using the following summarized search results (with citations), answer the original query. \" \\\n",
        "             \"Make sure to include proper citations for each piece of information.\\n\\n\" \\\n",
        "             \"Format your answer using Markdown syntax with proper headings, lists, and citation links. \" \\\n",
        "             \"Answer followed by citation.\"\n",
        "\n",
        "    for atomic, summaries in atomic_summaries.items():\n",
        "        prompt += f\"\\nAtomic Question: {atomic}\\n\"\n",
        "        for item in summaries:\n",
        "            summary = item.get('summary', '')\n",
        "            href = item.get('href', '')\n",
        "            prompt += f\"- Summary: {summary}\\n\"\n",
        "            prompt += f\"  Citation: {href}\\n\"\n",
        "        prompt += \"\\n\"\n",
        "\n",
        "    prompt += f\"Original Query: {original_query}\"\n",
        "    return flash_answer_(prompt)\n",
        "\n",
        "def process_query(user_query):\n",
        "    \"\"\"\n",
        "    Determines if a query requires search and breaks it into atomic questions if needed.\n",
        "    \"\"\"\n",
        "    system_instruction = \"\"\"\n",
        "    Today Date = 29-03-2025\n",
        "    You are an assistant that determines if a query requires internet search. Analyze the query and return a JSON with:\n",
        "    1. \"needs_search\": boolean - true if the query requires recent information (after November 2023) or specific facts\n",
        "    2. \"reason\": string - brief explanation why search is/isn't needed\n",
        "    3. \"atomic_questions\": array - if query is complex, break it into smaller atomic questions (empty if search not needed)\n",
        "\n",
        "    IMPORTANT: Any query about events, products, news, or data after November 2023 MUST have \"needs_search\" set to true.\n",
        "    If you are not familiar with the term asked in the question then also turn \"needs_search\" to true.\n",
        "    For small query return the original query. For complex and long queries requiring search, break them down into simpler 2-3 sub-questions.\n",
        "    Whenever someone asks about recent add 2025 in the sub questions.\n",
        "    \"\"\"\n",
        "\n",
        "    generation_config = {\n",
        "        \"temperature\": 0.2,\n",
        "        \"top_p\": 0.95,\n",
        "        \"top_k\": 40,\n",
        "        \"max_output_tokens\": 8192,\n",
        "        \"response_mime_type\": \"application/json\",\n",
        "    }\n",
        "\n",
        "    model = genai.GenerativeModel(\n",
        "        model_name=\"gemini-2.0-flash-001\",\n",
        "        generation_config=generation_config,\n",
        "        system_instruction=system_instruction,\n",
        "    )\n",
        "\n",
        "    chat_session = model.start_chat(history=[])\n",
        "    response = chat_session.send_message(user_query)\n",
        "\n",
        "    try:\n",
        "        result = json.loads(response.text)\n",
        "        return result\n",
        "    except json.JSONDecodeError:\n",
        "        # Fallback if response is not valid JSON\n",
        "        return {\n",
        "            \"needs_search\": True,\n",
        "            \"reason\": \"Failed to parse response, defaulting to search required\",\n",
        "            \"atomic_questions\": [user_query]\n",
        "        }\n",
        "\n",
        "def process_with_full_search(original_query: str, process_query_result: dict, progress_callback=None) -> dict:\n",
        "    \"\"\"\n",
        "    If search is not required (needs_search is false), directly answer the query\n",
        "    Otherwise, for each atomic sub-question, perform full search with content extraction and summarization,\n",
        "    then combine the summaries to answer the original query with citations.\n",
        "    \"\"\"\n",
        "    if not process_query_result.get(\"needs_search\", True):\n",
        "        if progress_callback:\n",
        "            progress_callback(\"Direct answer (no search needed)\")\n",
        "        answer = flash_answer(original_query)\n",
        "        return {\"answer\": answer}\n",
        "    else:\n",
        "        atomic_questions = process_query_result.get(\"atomic_questions\", [])\n",
        "        if progress_callback:\n",
        "            progress_callback(f\"Processing {len(atomic_questions)} atomic questions\")\n",
        "\n",
        "        atomic_summaries = process_all_atomic_questions(atomic_questions, progress_callback)\n",
        "\n",
        "        if progress_callback:\n",
        "            progress_callback(\"Generating final answer\")\n",
        "\n",
        "        final_ans = final_answer(original_query, atomic_summaries)\n",
        "        return {\"answer\": final_ans, \"atomic_summaries\": atomic_summaries}\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Enhanced UI Components with Markdown Support\n",
        "# ---------------------------------------------------------------------------\n",
        "def create_search_ui():\n",
        "    \"\"\"Create and display the search interface\"\"\"\n",
        "    display(HTML(css_style))\n",
        "\n",
        "    # App container\n",
        "    app_html = \"\"\"\n",
        "    <div class=\"app-container\">\n",
        "        <div class=\"header\">\n",
        "            <h1>AI-Powered Web Search</h1>\n",
        "            <p>Ask any question to get researched answers with citations</p>\n",
        "        </div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    display(HTML(app_html))\n",
        "\n",
        "    # Create widgets\n",
        "    query_input = widgets.Text(\n",
        "        description='Query:',\n",
        "        placeholder='Enter your question here...',\n",
        "        layout=widgets.Layout(width='80%')\n",
        "    )\n",
        "\n",
        "    search_button = widgets.Button(\n",
        "        description='Search',\n",
        "        button_style='primary',\n",
        "        icon='search'\n",
        "    )\n",
        "\n",
        "    debug_checkbox = widgets.Checkbox(\n",
        "        value=False,\n",
        "        description='Show debug info',\n",
        "        layout=widgets.Layout(width='auto')\n",
        "    )\n",
        "\n",
        "    output_area = widgets.Output()\n",
        "    status_area = widgets.Output()\n",
        "\n",
        "    # Display widgets\n",
        "    display(widgets.HBox([query_input, search_button]))\n",
        "    display(debug_checkbox)\n",
        "    display(status_area)\n",
        "    display(output_area)\n",
        "\n",
        "    # Progress updates\n",
        "    def update_status(message):\n",
        "        with status_area:\n",
        "            clear_output(wait=True)\n",
        "            status_html = f'<div class=\"status-message\"><div class=\"loader\"></div> {message}</div>'\n",
        "            display(HTML(status_html))\n",
        "\n",
        "    # Handle search button click\n",
        "    def on_search_button_clicked(b):\n",
        "        query = query_input.value.strip()\n",
        "        if not query:\n",
        "            with output_area:\n",
        "                clear_output(wait=True)\n",
        "                display(HTML('<div class=\"error-message\">Please enter a query</div>'))\n",
        "            return\n",
        "\n",
        "        with output_area:\n",
        "            clear_output(wait=True)\n",
        "\n",
        "        # Process the query\n",
        "        update_status(\"Analyzing your query...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            query_analysis = process_query(query)\n",
        "            show_debug = debug_checkbox.value\n",
        "\n",
        "            # Display query analysis if debug is enabled\n",
        "            if show_debug:\n",
        "                with output_area:\n",
        "                    analysis_json = json.dumps(query_analysis, indent=2)\n",
        "                    debug_html = f\"\"\"\n",
        "                    <div class=\"debug-info\" style=\"display: block;\">\n",
        "                        <h3>Query Analysis:</h3>\n",
        "                        <pre>{analysis_json}</pre>\n",
        "                    </div>\n",
        "                    \"\"\"\n",
        "                    display(HTML(debug_html))\n",
        "\n",
        "            if not query_analysis.get(\"needs_search\", True):\n",
        "                update_status(\"Generating direct answer (no search needed)...\")\n",
        "                result = process_with_full_search(query, query_analysis, update_status)\n",
        "\n",
        "                with output_area:\n",
        "                    # Convert Markdown to HTML\n",
        "                    answer_html = markdown2.markdown(result['answer'])\n",
        "                    time_taken = time.time() - start_time\n",
        "                    direct_html = f\"\"\"\n",
        "                    <div class=\"result-container\">\n",
        "                        <h2>Answer</h2>\n",
        "                        <div class=\"answer-box\">{answer_html}</div>\n",
        "                        <p><em>Answered directly without search (took {time_taken:.2f} seconds)</em></p>\n",
        "                    </div>\n",
        "                    \"\"\"\n",
        "                    display(HTML(direct_html))\n",
        "            else:\n",
        "                atomic_questions = query_analysis.get(\"atomic_questions\", [])\n",
        "                update_status(f\"Breaking down into {len(atomic_questions)} sub-questions...\")\n",
        "\n",
        "                result = process_with_full_search(query, query_analysis, update_status)\n",
        "\n",
        "                # Convert Markdown to HTML\n",
        "                answer_html_content = markdown2.markdown(result['answer'])\n",
        "                time_taken = time.time() - start_time\n",
        "\n",
        "                # Build the HTML in parts to avoid nested f-string issues\n",
        "                answer_html = f\"\"\"\n",
        "                <div class=\"result-container\">\n",
        "                    <h2>Answer</h2>\n",
        "                    <div class=\"answer-box\">{answer_html_content}</div>\n",
        "                \"\"\"\n",
        "\n",
        "                if show_debug and \"atomic_summaries\" in result:\n",
        "                    answer_html += \"\"\"\n",
        "                    <div class=\"atomic-container\">\n",
        "                        <h3>Research Process:</h3>\n",
        "                    \"\"\"\n",
        "\n",
        "                    for question, summaries in result[\"atomic_summaries\"].items():\n",
        "                        question_html = f\"\"\"\n",
        "                        <div class=\"atomic-question\">{question}</div>\n",
        "                        \"\"\"\n",
        "                        answer_html += question_html\n",
        "\n",
        "                        for summary in summaries:\n",
        "                            summary_text = summary['summary'].replace('\\n', '<br>')\n",
        "                            summary_href = summary['href']\n",
        "                            summary_title = summary['title'] or summary['href']\n",
        "\n",
        "                            summary_html = f\"\"\"\n",
        "                            <div class=\"summary-item\">\n",
        "                                <div>{summary_text}</div>\n",
        "                                <div class=\"citation\">Source: <a href=\"{summary_href}\" target=\"_blank\">{summary_title}</a></div>\n",
        "                            </div>\n",
        "                            \"\"\"\n",
        "                            answer_html += summary_html\n",
        "\n",
        "                    answer_html += \"</div>\"\n",
        "\n",
        "                footer_html = f\"\"\"\n",
        "                    <p><em>Researched answer (took {time_taken:.2f} seconds)</em></p>\n",
        "                </div>\n",
        "                \"\"\"\n",
        "                answer_html += footer_html\n",
        "\n",
        "                with output_area:\n",
        "                    display(HTML(answer_html))\n",
        "\n",
        "        except Exception as e:\n",
        "            with output_area:\n",
        "                error_message = str(e)\n",
        "                error_html = f\"\"\"\n",
        "                <div class=\"error-message\">\n",
        "                    <h3>Error occurred:</h3>\n",
        "                    <p>{error_message}</p>\n",
        "                </div>\n",
        "                \"\"\"\n",
        "                display(HTML(error_html))\n",
        "\n",
        "        with status_area:\n",
        "            clear_output()\n",
        "\n",
        "    search_button.on_click(on_search_button_clicked)\n",
        "\n",
        "    # Handle Enter key in query input\n",
        "    def on_enter(widget):\n",
        "        on_search_button_clicked(None)\n",
        "\n",
        "    query_input.on_submit(on_enter)\n",
        "\n",
        "    return {\n",
        "        'query_input': query_input,\n",
        "        'search_button': search_button,\n",
        "        'output_area': output_area,\n",
        "        'status_area': status_area\n",
        "    }\n",
        "\n",
        "# Function to run the app\n",
        "def run_search_app():\n",
        "    \"\"\"Main function to run the search application\"\"\"\n",
        "    # Check if API key is set\n",
        "    if not api_key or api_key == \"YOUR_API_KEY_HERE\":\n",
        "        api_key_warning = \"\"\"\n",
        "        <div style=\"color: red; padding: 10px; background-color: #ffe6e6; border-radius: 5px; margin: 10px 0;\">\n",
        "            <h3>âš ï¸ API Key Missing</h3>\n",
        "            <p>Please set your Google AI API key in the code before running.</p>\n",
        "            <code>api_key = \"your_actual_api_key_here\"</code>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "        display(HTML(api_key_warning))\n",
        "        return\n",
        "\n",
        "    # Create and display UI\n",
        "    ui_components = create_search_ui()\n",
        "\n",
        "    print(\"ðŸ‘† Search app is ready to use!\")\n",
        "\n",
        "# Run the app when executed\n",
        "if __name__ == \"__main__\" or 'google.colab' in str(get_ipython()):\n",
        "    run_search_app()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708,
          "referenced_widgets": [
            "608af66e996f4e389d55e08aa02971b3",
            "7d80706405e74dbeb91a3204ede941ec",
            "b819673a673e490fa99a9f6d639ed82a",
            "46d65d987b094e39901960d20f09686f",
            "914b244d319840f3b6674df319395b73",
            "88c397912b834b5197bf20df24a0f8a7",
            "b1e9c5fc190c4f5d8e701b6c69e2dbb4",
            "b7b827ffc5e14b8c90e170b85e2de3d4",
            "0391294194814088a4ffc0389edf2efe",
            "f4d8be538b804f09b5124cde57cb5256",
            "f7d797db76a4403ca58118d15f90d5dd",
            "4fefe1100ada4df8be3c09a20df6e35c",
            "05ced2337fc24335af44f073287fd27a",
            "af4556fe8e0d4906ad6ff7f306a6ca59",
            "d4fbd28a15d5471b9b62e180bc724aee"
          ]
        },
        "id": "E4jv-nipjVCz",
        "outputId": "1ecbb51b-7f5c-42bc-bfc3-2b6c1bf71709"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    .app-container {\n",
              "        font-family: 'Roboto', sans-serif;\n",
              "        max-width: 1000px;\n",
              "        margin: 0 auto;\n",
              "        padding: 20px;\n",
              "        background-color: #f9f9f9;\n",
              "        border-radius: 10px;\n",
              "        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);\n",
              "    }\n",
              "    .header {\n",
              "        text-align: center;\n",
              "        margin-bottom: 20px;\n",
              "        color: #2c3e50;\n",
              "    }\n",
              "    .search-container {\n",
              "        margin-bottom: 20px;\n",
              "    }\n",
              "    .result-container {\n",
              "        background-color: white;\n",
              "        padding: 15px;\n",
              "        border-radius: 8px;\n",
              "        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);\n",
              "        margin-top: 20px;\n",
              "    }\n",
              "    .answer-box {\n",
              "        background-color: #e8f4f8;\n",
              "        padding: 15px;\n",
              "        border-radius: 8px;\n",
              "        border-left: 5px solid #3498db;\n",
              "        margin-top: 10px;\n",
              "    }\n",
              "    /* Added styles for Markdown */\n",
              "    .answer-box h1, .answer-box h2, .answer-box h3 {\n",
              "        color: #2c3e50;\n",
              "        margin-top: 10px;\n",
              "        margin-bottom: 10px;\n",
              "    }\n",
              "    .answer-box ul, .answer-box ol {\n",
              "        margin-left: 20px;\n",
              "    }\n",
              "    .answer-box code {\n",
              "        background-color: #f0f0f0;\n",
              "        padding: 2px 4px;\n",
              "        border-radius: 3px;\n",
              "    }\n",
              "    .answer-box pre {\n",
              "        background-color: #f0f0f0;\n",
              "        padding: 10px;\n",
              "        border-radius: 5px;\n",
              "        overflow-x: auto;\n",
              "    }\n",
              "    .atomic-container {\n",
              "        margin-top: 20px;\n",
              "        padding: 10px;\n",
              "        background-color: #f5f5f5;\n",
              "        border-radius: 8px;\n",
              "    }\n",
              "    .atomic-question {\n",
              "        font-weight: bold;\n",
              "        color: #2980b9;\n",
              "        margin-top: 10px;\n",
              "    }\n",
              "    .summary-item {\n",
              "        background-color: white;\n",
              "        padding: 10px;\n",
              "        margin: 8px 0;\n",
              "        border-radius: 6px;\n",
              "        border-left: 3px solid #27ae60;\n",
              "    }\n",
              "    .citation {\n",
              "        font-size: 0.8em;\n",
              "        color: #7f8c8d;\n",
              "        margin-top: 5px;\n",
              "    }\n",
              "    .progress-container {\n",
              "        margin-top: 20px;\n",
              "        text-align: center;\n",
              "    }\n",
              "    .status-message {\n",
              "        margin: 15px 0;\n",
              "        color: #2c3e50;\n",
              "        font-style: italic;\n",
              "    }\n",
              "    .debug-info {\n",
              "        font-family: monospace;\n",
              "        font-size: 0.8em;\n",
              "        background-color: #f0f0f0;\n",
              "        padding: 10px;\n",
              "        border-radius: 5px;\n",
              "        margin-top: 20px;\n",
              "        display: none;\n",
              "    }\n",
              "    .loader {\n",
              "        display: inline-block;\n",
              "        width: 30px;\n",
              "        height: 30px;\n",
              "        border: 3px solid rgba(0,0,0,.3);\n",
              "        border-radius: 50%;\n",
              "        border-top-color: #3498db;\n",
              "        animation: spin 1s ease-in-out infinite;\n",
              "    }\n",
              "    @keyframes spin {\n",
              "        to { transform: rotate(360deg); }\n",
              "    }\n",
              "    .error-message {\n",
              "        color: #e74c3c;\n",
              "        padding: 10px;\n",
              "        background-color: #fadbd8;\n",
              "        border-radius: 5px;\n",
              "        margin: 10px 0;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div class=\"app-container\">\n",
              "        <div class=\"header\">\n",
              "            <h1>AI-Powered Web Search</h1>\n",
              "            <p>Ask any question to get researched answers with citations</p>\n",
              "        </div>\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(Text(value='', description='Query:', layout=Layout(width='80%'), placeholder='Enter your questiâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "608af66e996f4e389d55e08aa02971b3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Checkbox(value=False, description='Show debug info', layout=Layout(width='auto'))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0391294194814088a4ffc0389edf2efe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4fefe1100ada4df8be3c09a20df6e35c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af4556fe8e0d4906ad6ff7f306a6ca59"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ‘† Search app is ready to use!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:backoff:Backing off flash_answer(...) for 0.4s (requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#not needed"
      ],
      "metadata": {
        "id": "4hMPKkiadWRu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "8i_7wlihdQ9b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBQTVcd7nbyk"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from typing import List, Dict, Optional, Union\n",
        "\n",
        "def search_ddg(\n",
        "    query: str,\n",
        "    search_type: str = \"search\",\n",
        "    max_results: int = 5,\n",
        "    timeout: int = 10\n",
        ") -> List[Dict]:\n",
        "    \"\"\"\n",
        "    A simple function to search DuckDuckGo and return results.\n",
        "\n",
        "    Args:\n",
        "        query (str): The search query\n",
        "        search_type (str): Type of search - \"search\" or \"news\"\n",
        "        max_results (int): Maximum number of results to return\n",
        "        timeout (int): Request timeout in seconds\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: List of search results as dictionaries\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from duckduckgo_search import DDGS\n",
        "    except ImportError:\n",
        "        print(\"Error: 'duckduckgo-search' package not installed!\")\n",
        "        print(\"Install it with: pip install duckduckgo-search\")\n",
        "        return []\n",
        "\n",
        "    # Create DDGS instance\n",
        "    ddgs = DDGS(timeout=timeout)\n",
        "\n",
        "    # Perform the search based on search_type\n",
        "    if search_type.lower() == \"news\":\n",
        "        return ddgs.news(keywords=query, max_results=max_results)\n",
        "    else:\n",
        "        return ddgs.text(keywords=query, max_results=max_results)\n",
        "\n",
        "\n",
        "def simple_ddg_search(\n",
        "    query: str,\n",
        "    search_type: str = \"search\",\n",
        "    max_results: int = 5,\n",
        "    return_json: bool = False\n",
        ") -> Union[List[Dict], str]:\n",
        "    \"\"\"\n",
        "    Even simpler wrapper function for DuckDuckGo searches.\n",
        "\n",
        "    Args:\n",
        "        query (str): What to search for\n",
        "        search_type (str): \"search\" or \"news\"\n",
        "        max_results (int): How many results to return\n",
        "        return_json (bool): If True, returns JSON string instead of Python list\n",
        "\n",
        "    Returns:\n",
        "        Union[List[Dict], str]: Search results as list of dicts or JSON string\n",
        "    \"\"\"\n",
        "    results = search_ddg(query, search_type, max_results)\n",
        "\n",
        "    if return_json:\n",
        "        return json.dumps(results, indent=2)\n",
        "    return results\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Example 1: Basic search returning Python objects\n",
        "    results = simple_ddg_search(\"Government schemes leveraging NGO networks for implementation in india\", max_results=10)\n",
        "    print(\"\\nSearch Results:\")\n",
        "    for i, result in enumerate(results, 1):\n",
        "        print(result)\n",
        "    # Example 2: News search with JSON output\n",
        "    news_json = simple_ddg_search(\"Government schemes leveraging NGO networks for implementation in india  \",search_type= \"news\", return_json=True)\n",
        "\n",
        "    print(f\"\\nNews Results (JSON):\\n{news_json}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import google.generativeai as genai\n",
        "from datetime import datetime\n",
        "\n",
        "def process_query_(user_query):\n",
        "    # Configure the API\n",
        "    genai.configure(api_key=\"AIzaSyDotanrPA6I0H1WZNPlL1e50gtV-oMR2WM\")\n",
        "\n",
        "    # Define system instruction to determine if search is needed\n",
        "    system_instruction_ = \"\"\"\n",
        "summmerise based on the question given\n",
        "    \"\"\"\n",
        "\n",
        "    # Create the model with appropriate configuration\n",
        "    generation_config = {\n",
        "        \"temperature\": 0.2,  # Lower temperature for more deterministic results\n",
        "        \"top_p\": 0.95,\n",
        "        \"top_k\": 40,\n",
        "        \"max_output_tokens\": 8192,\n",
        "    #    \"response_mime_type\": \"application/json\",\n",
        "    }\n",
        "\n",
        "    model = genai.GenerativeModel(\n",
        "        model_name=\"gemini-1.5-flash-002\",\n",
        "        generation_config=generation_config,\n",
        "        system_instruction=system_instruction_,\n",
        "    )\n",
        "\n",
        "    # Start chat and send user query\n",
        "    chat_session = model.start_chat(history=[])\n",
        "    response = chat_session.send_message(user_query)\n",
        "\n",
        "    # Parse the response\n",
        "    try:\n",
        "        result = json.loads(response.text)\n",
        "        return result\n",
        "    except json.JSONDecodeError:\n",
        "        # Fallback if response is not valid JSON\n",
        "        return response.text\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    query = answer\n",
        "    result = process_query_(query)\n",
        "    print(json.dumps(result, indent=2))"
      ],
      "metadata": {
        "id": "m--zH6Z4uVaw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "428b765c-ca42-4835-96a0-bcb00e79f881"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Child labor remains a significant problem globally, particularly in Africa and South America.  South Sudan has the highest rate (48%), followed by Ethiopia (45%), Burkina Faso (42%), and Cameroon and Chad (both 39%).  While many countries have high rates, some like Barbados and Saint Lucia report very low rates (1%).  The data shows a disparity in child labor participation between genders in several countries.\\n\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YRwMLz8gWTH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bring full content"
      ],
      "metadata": {
        "id": "PVjh4pfMiurP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from duckduckgo_search import DDGS\n",
        "\n",
        "def get_full_content(url):\n",
        "    \"\"\"Fetch the full content of a webpage\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Basic extraction - you might need to refine this based on site structure\n",
        "        paragraphs = soup.find_all('p')\n",
        "        full_text = ' '.join([p.get_text() for p in paragraphs])\n",
        "        return full_text\n",
        "    except Exception as e:\n",
        "        return f\"Error fetching content: {str(e)}\"\n",
        "\n",
        "def search_with_full_content(query, max_results=1):\n",
        "    \"\"\"Search and retrieve full content for each result\"\"\"\n",
        "    ddgs = DDGS()\n",
        "    results = ddgs.text(keywords=query, max_results=max_results)\n",
        "\n",
        "    enhanced_results = []\n",
        "    for result in results:\n",
        "        # Add full content to each result\n",
        "        result['full_content'] = get_full_content(result['href'])\n",
        "        enhanced_results.append(result)\n",
        "\n",
        "    return enhanced_results"
      ],
      "metadata": {
        "id": "edO4Q0alh-EB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a"
      ],
      "metadata": {
        "id": "_y4rdRaC8VsK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b910e667-ca93-44d6-9c52-a5f75e594933"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'title': 'Child Labor by Country 2025 - World Population Review',\n",
              "  'href': 'https://worldpopulationreview.com/country-rankings/child-labor-by-country',\n",
              "  'body': 'Child labor is a controversial practice and is often illegal in many countries. It involves using children between the ages of 5 and 17 years old who are used for labor in a commercial or business setting.',\n",
              "  'full_content': '48%  45%  42%  39%  39%  Child labor is a controversial practice and is often illegal in many countries. It involves using children between the ages of 5 and 17 years old who are used for labor in a commercial or business setting. However, just because it is frowned upon, that doesnÃ¢\\x80\\x99t mean that it isnÃ¢\\x80\\x99t still a popular practice around the globe. In this article, we will take a closer look at which countries still rely heavily on child labor and how the numbers stack up among one another. The majority of the countries that still participate in child labor practices are located in either Africa or South America. At least 47 countries, including Ethiopia, Cameroon, Chad, Togo, Madagascar, Laos, Zambia, Nepal, Ivory Coast, Kyrgyzstan, Uzbekistan, Sudan, Fiji, Peru, Honduras, Mongolia, Bolivia, Samoa, Afghanistan, Mali, and Cambodia still use child labor regularly. The country that uses child labor the most in the world is South Sudan, with a score of 48 (percentage of children who have ever engaged in child labor), with 50% of male and 47% of female children involved. The country with the second highest number of child laborers is Ethiopia. The country has a rating of 45 with males outpacing females 51 to 39. The next country on the list is Burkina Faso. The country has a score of 42. Here, the ratio of female to male child labor workers is a little more split, with males just outpacing females 43 to 40. Cameroon and Chad share the next place on the list with a score of 39 each. They are also nearly evenly split between males and females with scores of 40:38 and 39:40, respectively. Madagascar is next on the list with a score of 37, which is divided into a ratio of 35 females to 38 males. Haiti follows that score with 36. In Haiti, the number of child labor workers skews drastically in the male direction with a ratio of 44 males to 26 females. There are also several countries that have very low child labor numbers. Barbados and Saint Lucia share a score of 1. Brazil, Argentina, Panama, Georgia, Tunisia, Jordan, and Palestine all have ratings of just 2. Several countries have a rating of 3 when it comes to child labor workers, including Belize, North Macedonia, Jamaica, Albania, and Algeria. Egypt, Philippines, Ukraine, Uzbekistan, North Korea, Dominican Republic, Belarus, Costa Rica, Armenia, and Bhutan all have child labor rates of 4. Turkmenistan, even though the practice is legal in the country, currently has a score of only 5%.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Function Calling](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb)"
      ],
      "metadata": {
        "id": "4_34uODUU2o5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChromaDb"
      ],
      "metadata": {
        "id": "kw6p1Kk1cgER"
      }
    }
  ]
}